{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling sentiment analysis in MXNet with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MLP\n",
    "\n",
    "A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network\n",
    "\n",
    "<center><img src='images/mlp.svg' width=700></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MXNet\n",
    "\n",
    "Apache MXnet is an open-source machine learning framework. It allows to build, train, and port deep learning models. MXNet is supported by AWS, Intel, Microsoft as well as educational instituions such as  MIT, Carnegie Mellon, and University of Washington. MXNet combines flexibility, high performance, and scalability which make it a great choice for many machine learning use cases. MXNet also have Gluon library which implements clear and intuitive API for deep learning models without sacrificing performance and scalability. \n",
    "\n",
    "Over the course of these labs, you'll learn how to use MXnet, Gluon, and specialized toolkits to solve variety of machine learning problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "\n",
    "Sentiment analysis is the use of natural language processing (NLP) to determine the attitude expressed by an author in a piece of written text towards a topic, e.g. movie review. The attitude can be positive, neutral, and negative.\n",
    "From a machine learning perspective, sentiment analysis can be treated as a classification problem. In the tutorial, we will train an MLP based model for sentiment analysis.\n",
    "While there are other algorithms, such as Recurrent Neural Networks (RNNs), that are better at capturing the syntactic structure of the sentence for sentiment analysis, MLP is a straight and simple network that is quick to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use This TutorialÂ¶\n",
    "You can use this tutorial by executing each snippet of python code in order as it appears in the notebook. An easy way to do so is to click on the \"run cell, select below\" arrow that is to the left of the \"stop\" icon in the toolbar. In this tutorial, we will train an MLP on an IMDB dataset which will ultimately produce a neural network that can predict the sentiment of movie reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "- Skills: Familiarity with MXNet, Python, Numpy, basics of MLP networks.\n",
    "- Resource: Sagemaker Notebook instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "The training and testing dataset is the IMDB movie review database.  It contains a total of 50,000 movie reviews that are tagged (labeled) with either a negative (0) or a positive (1) sentiment.  We will split the dataset into 35,000 reviews for training, 10,050 for validation, and 4,950 reviews for testing. Refer to official [dataset documentation](https://ai.stanford.edu/~amaas/data/sentiment/) for more details.\n",
    "\n",
    "Below, we download locally IMDB dataset and un-archive it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from public sources ~3 minutes\n",
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf aclImdb_v1.tar.gz # un-archive dataset ~3 minutes\n",
    "!rm -r aclImdb_v1.tar.gz # deleting tar.gz \n",
    "!ls -l aclImdb # let's peek inside un-archived dataset and confirm that it's there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the libraries and modules\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from text import Tokenizer\n",
    "from matplotlib import pyplot\n",
    "from six.moves.urllib.request import urlopen\n",
    "from sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from IPython.display import display \n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Enable logging so we will see output during the training\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# import MXNet packages\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn, loss, Trainer\n",
    "from mxnet import init\n",
    "from mxnet.gluon.contrib.estimator import estimator\n",
    "from mxnet.gluon.contrib.estimator.event_handler import TrainBegin, TrainEnd, EpochEnd, CheckpointHandler # this is optional\n",
    "from mxnet.gluon.data import DataLoader, ArrayDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Movie Review Data\n",
    "\n",
    "The raw reviews are in the aclImdb directory.  We will process the unzipped raw reviews into training and test datasets for training and validation purpose. Additionally, we need to convert text into machine-friendly format. For sentimental analysis probelm, it's simple and computationally cheap to represent all words as a vocabulary consisting of {index: word} pairs (e.g. {0 : \"movie\"}). Then, we'll use word indices as input to our model. The process of representing words as numerical values is called \"encoding\".\n",
    "\n",
    "## LAB INSTRUCTION\n",
    "- Enter **10000** as the value for the **vocabsize** variable.  This is limit on size of word vocabulary.  Any word outside of vacabulary will be encoded as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify number of words to index and this is also the size of vocabulary\n",
    "vocabsize =  #Follow instruction above\n",
    "\n",
    "# This is the directory where the raw review data is located\n",
    "path = \"aclImdb/\"\n",
    "\n",
    "# List all the files for the reviews in the following directories\n",
    "ff = [path + \"train/pos/\" + x for x in os.listdir(path + \"train/pos\")] + \\\n",
    "     [path + \"train/neg/\" + x for x in os.listdir(path + \"train/neg\")] + \\\n",
    "     [path + \"test/pos/\" + x for x in os.listdir(path + \"test/pos\")] + \\\n",
    "     [path + \"test/neg/\" + x for x in os.listdir(path + \"test/neg\")]\n",
    "\n",
    "# Find all HTML tags using following regex pattern\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "# Remove all found HTML tags\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "input_label = ([1] * 12500 + [0] * 12500) * 2\n",
    "input_text  = []\n",
    "\n",
    "for f in ff:\n",
    "    with open(f) as fin:\n",
    "        pass\n",
    "        input_text += [remove_tags(\" \".join(fin.readlines()))]\n",
    "            \n",
    "# Initialize a tokenizer with the vocabulary size and train on data input text to create a vocabulary for all \n",
    "# the unique words found in the text inputs\n",
    "tok = Tokenizer(vocabsize)\n",
    "tok.fit_on_texts(input_text)\n",
    "\n",
    "        \n",
    "# Create training (60% of review), validation(30% of reviews), and testing(10% reviews) datasets.  \n",
    "# Words will be replaced with indexes for the words.\n",
    "tok_input_text = tok.texts_to_sequences(input_text)\n",
    "X_train, X_val, y_train, y_val = train_test_split(tok_input_text, input_label, test_size = 0.3, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.33, random_state=1)\n",
    "\n",
    "print(\"Reviews in training dataset %d\" % len(X_train))\n",
    "print(\"Reviews in validations dataset %d\" %len(X_val))\n",
    "print(\"Reviews in training dataset %d\" % len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the basic metrics of the datasets including the number of unique words, unique label values, and the mean and standard deviation of the data set.\n",
    "\n",
    "## LAB INSTRUCTION:\n",
    "- Anwer the following questions\n",
    "   - What are the unique label values?\n",
    "   - What is the mean size of all the review texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some analysis of the data\n",
    "\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "\n",
    "# Summarize review length\n",
    "print(\"Number of unique words : %i\" % len(np.unique(np.hstack(X))))\n",
    "print ('')\n",
    "print (\"Label value\")\n",
    "print (np.unique(y_train))\n",
    "print ('')\n",
    "print(\"Review length: \")\n",
    "\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words with %f standard deviation\" % (np.mean(result), np.std(result)))\n",
    "\n",
    "# plot review length distribution\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data Processing\n",
    "\n",
    "We will pad the data to a fixed length and create [MXNet DataLoader objects](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.data.DataLoader.html) to be used for training later.\n",
    "\n",
    "## LAB INSTRUCTIONS\n",
    "- Enter **500** as the value for the **maxtextlen** variable.\n",
    "- Enter **250** as the value for the **Batch_Size** variable.\n",
    "- Answer the questions below\n",
    "    - Why do you think we want the maximum length to be 500?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum text length for each review in the training data\n",
    "maxtextlen =    #Follow instruction above\n",
    "Batch_Size =    #Follow instruction above\n",
    "\n",
    "# Specify the maximum length of the reviews we want to process and pad the training and test data \n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxtextlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxtextlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxtextlen)\n",
    "\n",
    "# convert list to nd array type as mxnet.gluon.data.ArrayDataset takes Numpy array data type\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "# Create DataLoaders which return batches of features and laberls during training and validation processes. \n",
    "# Please note that datasets are shuffled to ensure randomness of data.\n",
    "train_data = ArrayDataset(X_train,y_train)\n",
    "train_data = DataLoader(train_data, batch_size=Batch_Size, shuffle=True)\n",
    "\n",
    "val_data = ArrayDataset(X_val, y_val)\n",
    "val_data = DataLoader(val_data, batch_size=Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Sample Input Data\n",
    "\n",
    "## LAB INSTRUCTION\n",
    "- Answer the following questions:\n",
    "    - What does each integer represent in the vector?\n",
    "    - What is the length of the vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also take a look at 1 row of the training data\n",
    "# The integers represent a word in the original text \n",
    "print ('Review Example - Coded with word index')\n",
    "print (X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an MLP Network\n",
    "\n",
    "We will build a simple MLP network with 2 hidden layers to determine negative and positive sentiment of movie review. \n",
    "\n",
    "## Understanding model output\n",
    "As we need to predict binary category (positive or negative sentiment), our output layer has only 2 units (to represent each category). The numeric values in these two units will determine whether network predict positive or negative sentiment for a given review. \n",
    "\n",
    "To make model outputs more convenient, we apply to them [Softmax function](https://en.wikipedia.org/wiki/Softmax_function) which takes as input a vector of any real numbers and normalizes this vector into probability distribution with following properties:\n",
    "- sum of probabilities will add to 1;\n",
    "- all probabilities will be within (0,1).\n",
    "\n",
    "This allows us to take any model output for K categories and represent it as probability of each K categories to be true.\n",
    "\n",
    "## Training model using loss function\n",
    "Machine Learning models are in most cases using loss function to train. Loss function takes model predictions and true labels and scores how far predictions are true labels. If you have a good model and data, then during training you'll see that your loss score is reducing which indicates that models is learning from data and making more accurate predictions over time. For binary categorical problem such as sentiment analysis, it's common to use [cross-entropy loss function](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) which compares distribution of true labels with model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP network using MXNet Gluon library\n",
    "\n",
    "# We define a new function build_net which takes Gluon block as an input and stacks layers on top of it.\n",
    "# Function returns MLP network ready for training.\n",
    "def build_net(net):\n",
    "    with net.name_scope():\n",
    "        # We embed the integer representation for each word into a vector of size 32.\n",
    "        # Embedding is a technique that places related words close together in vector space.\n",
    "        # This helps improve the accuracy of model.\n",
    "        # input_dim is the size of the vocabulary.  output_dim is the dimension of the output embedded vector.\n",
    "        net.add(nn.Embedding(input_dim=vocabsize, output_dim=32))\n",
    "\n",
    "        # The output from the embedding layer will be dimensional matrix, since MLP only accepts 1 dimensional vector, \n",
    "        # we need to flatten it back to one dimension vector\n",
    "        net.add(nn.Flatten())\n",
    "\n",
    "        # We create a fully connected layer (in other words densily connected) with 250 neurons.  \n",
    "        # This layer will take the flattened input and perform a linear calculation on the input data f(x) = â¨w, xâ© + b\n",
    "        # please note, we specify \"Relu\" activation, so MLP model can \"learn\" non linear data patterns.\n",
    "        net.add(nn.Dense(units=250, activation=\"relu\"))\n",
    "\n",
    "\n",
    "        # Choose if you want to want to have Dropout layer to introduce regularization. \n",
    "        # If dropout rate is '0' than Dropout layer won't be added. \n",
    "        dropout_rate = 0.5 \n",
    "        if dropout_rate > 0.0:\n",
    "            net.add(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # We add another hidden layer with 2 hidden units as we have 2 desired output (1, 0) - positive or negative review.\n",
    "        net.add(nn.Dense(units=2))\n",
    "        \n",
    "        return net\n",
    "\n",
    "# MLP model is simple feed forward network architecture, \n",
    "# so we'll use Gluon Sequential block, which stacks layers sequentially one on top of another.\n",
    "net = build_net(nn.Sequential())\n",
    "\n",
    "# mxnet.gluon.loss.SoftmaxCrossEntropyLoss includes softmax function. \n",
    "# Therefore, we didn't include \"softmax\" activation into output Dense layer above.\n",
    "softmax_cross_entropy = loss.SoftmaxCrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Now we are ready to train the model.  We also need to define some hyper-parameters for model training.\n",
    "\n",
    "## LAB INSTRUCTION\n",
    "- Enter **10** as the value for variable **num_epoch**  - (This is number of epochs to train the model)\n",
    "- Enter **\"adam\"** as the value for variable **optimizer** - (This is the optimizer for updating the weights)\n",
    "- Enter **mx.metric.Accuracy()** as the value for variable **eval_metric**  (This is the performance evaluation metric)\n",
    "- Enter **0.01** as the value for variable **learning_rate** - (This parameters defines how much we are adjusting weights of our network during training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameteres \n",
    "num_epoch =        # Follow instruction above \n",
    "optimizer =        # Follow instruction above\n",
    "eval_metric =      # Follow instruction above\n",
    "learning_rate =    # Follow instruction above\n",
    "\n",
    "# MXNet allows users to choose whether to run computation on GPU or CPU devices. \n",
    "# Code line below defaults to computation on GPU if it's available.\n",
    "device = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()\n",
    "\n",
    "# To successfully train our model, we need to initialize model parameters (weights and biases).\n",
    "# We use normal distribution for parameters using from mx.init package\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), force_reinit=True, ctx=device)\n",
    "\n",
    "# model training\n",
    "trainer = Trainer(net.collect_params(), optimizer, {'learning_rate': learning_rate})\n",
    "\n",
    "\n",
    "# Define the estimator, by passing to it the model, loss function, metrics, trainer object and context\n",
    "est = estimator.Estimator(net=net,\n",
    "                          loss=softmax_cross_entropy,\n",
    "                          val_metrics=eval_metric,\n",
    "                          trainer=trainer,\n",
    "                          context=device)\n",
    "\n",
    "est.fit(train_data=train_data, val_data=val_data, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB INSTRUCTION\n",
    "\n",
    "- Add a new cell by clicking on the **\"+\"** sign on the tool bar  \n",
    "- Type **net.summary(nd.ones((2500,500), ctx=device))** in the new cell and run the cell to visualize the network\n",
    "\n",
    "Answer following questions:\n",
    "- What are params of Embedding and Dense layers? What happened with layer params during model training?\n",
    "- Why does Activation and Dropout layers have no any params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.summary(nd.ones((2500,500), ctx=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "We evaluated the model during training using validation dataset.  Now let's try to evaluate accuracy on test dataset which was \"unseen\" by model during training.\n",
    "\n",
    "Answer following questions:\n",
    "- Is model accuracy on test dataset comparable to model accuracy on training and validation datasets? \n",
    "- How can you explain difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert evaluation data and labels into MXNet NDArray class as Gluon model expects it.\n",
    "labels = mx.nd.array(y_test, ctx=device)\n",
    "test_data = mx.nd.array(X_test, ctx=device)\n",
    "\n",
    "# Get prediction using trained model and doing one pass forward using net(x) method\n",
    "predictions = net(test_data)\n",
    "# Convert float model output to binary values: 0 (negative) or 1 (positive)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "# Create MXnet Metric object to evaluate accuracy of predictions.\n",
    "metric = mx.metric.Accuracy(axis = 0)\n",
    "metric.update(preds = predictions, labels = labels)\n",
    "print(metric.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Model\n",
    "\n",
    "Now we have the model fully trained, we can save the model for later use.\n",
    "\n",
    "\n",
    "## LAB INSTRUCTION\n",
    "- After running the cell below, check that file with parameters was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = \"mpl.params\"\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Making Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let create a new network with exact same network architecture and load previously trained parameters.\n",
    "new_net = build_net(nn.Sequential())\n",
    "new_net.load_parameters(filename, ctx=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper function for making the prediction\n",
    "\n",
    "# This function takes a text string and return a nd array with word indexes \n",
    "def prepare_imdb_list(text, maxlen=500, vocabsize=10000):\n",
    "    imdb_word_index = tok.word_index\n",
    "    \n",
    "    sentence = []\n",
    "\n",
    "    sentence.append(str(text))\n",
    "    \n",
    "\n",
    "    #tokenize the input sentence\n",
    "    tokens = Tokenizer()\n",
    "    tokens.fit_on_texts(sentence)\n",
    "\n",
    "    # get a list of words from the encoding\n",
    "    words = []\n",
    "    for iter in range(len(tokens.word_index)):\n",
    "        words += [key for key,value in tokens.word_index.items() if value==iter+1]\n",
    "    \n",
    "    # create a imdb based sequence from the words and specified vocab size\n",
    "    imdb_seq = []\n",
    "    err_count = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            idx = imdb_word_index[w]\n",
    "            if idx < vocabsize:\n",
    "                imdb_seq.append(idx)\n",
    "        except:\n",
    "            err_count = err_count + 1\n",
    "\n",
    "    # next we need to create a list of list so we can use pad_sequence to pad the inputs\n",
    "    new_list = []\n",
    "    new_list.append(imdb_seq)\n",
    "\n",
    "    new_list = pad_sequences(new_list, maxlen=maxlen)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def predict_sentiment(model, text_nd):\n",
    "    # Convert input data into expected MXNet NDArray format\n",
    "    pred_data = mx.nd.array(text_nd, ctx=device)\n",
    "    \n",
    "    # Get prediction using trained model and doing one pass forward using net(x) method\n",
    "    predictions = model(pred_data)\n",
    "    predictions = nd.softmax(predictions, axis=1) # convert float model output into softmax probabilities\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Movie Review Text For Testing\n",
    "\n",
    "You can use the samples below - or any other review text - to try out the predictive power of the model. \n",
    "\n",
    "## Negative sentiment review samples\n",
    "- Blake Edwards' legendary fiasco, begins to seem pointless after just 10 minutes. A combination of The Eagle Has Landed, Star!, Oh! What a Lovely War!, and Edwards' Pink Panther films, Darling Lili never engages the viewer; the aerial sequences, the musical numbers, the romance, the comedy, and the espionage are all ho hum. At what point is the viewer supposed to give a damn? This disaster wavers in tone, never decides what it wants to be, and apparently thinks it's a spoof, but it's pathetically and grindingly square. Old fashioned in the worst sense, audiences understandably stayed away in droves. It's awful. James Garner would have been a vast improvement over Hudson who is just cardboard, and he doesn't connect with Andrews and vice versa. And both Andrews and Hudson don't seem to have been let in on the joke and perform with a miscalculated earnestness. Blake Edwards' SOB isn't much more than OK, but it's the only good that ever came out of Darling Lili. The expensive and professional look of much of Darling Lili, only make what it's all lavished on even more difficult to bear. To quote Paramount chief Robert Evans, 24 million dollars worth of film and no picture.\n",
    "\n",
    "- A mean spirited, repulsive horror film about 3 murderous children. Susan Strasberg is totally wasted in a 5-minute cameo, even though she receives star billing. If you are a Julie Brown fan, you'll want to check it out, since she's naked in a couple of shots. All others,avoid.\n",
    "\n",
    "\n",
    "## Positive sentiment review samples\n",
    "- I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
    "\n",
    "- This is one of my three all-time favorite movies. My only quibble is that the director, Peter Yates, had too many cuts showing the actors individually instead of together as a scene, but the performances were so great I forgive him. Albert Finney and Tom are absolutely marvelous; brilliant. The script is great, giving a very good picture of life in the theatre during World War II (and, therefore, what it was like in the 30s as well). Lots of great, subtle touches, lots of broad, overplayed strokes, all of it perfectly done. Scene after scene just blows me away, and then there's the heartbreaking climax.\n",
    "\n",
    "Enter your review in variable **text_to_predict_** below and then run next cell to predict this text snippet sentiment. See example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_predict = \"\"\"\n",
    "I went and saw this movie last night after being coaxed to by a few friends of mine. \n",
    "I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was \n",
    "only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well,\n",
    "and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is \n",
    "that it can toy with our emotions. This one did exactly that. The entire theater (which was sold \n",
    "out) was overcome by laughter during the first half of the movie, and were moved to tears \n",
    "during the second half. While exiting the theater I not only saw many women in tears, \n",
    "but many full grown men as well, trying desperately not to let anyone see them crying. \n",
    "This movie was great, and I suggest that you go see it before you judge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nd = prepare_imdb_list(text_to_predict)   \n",
    "predictions = predict_sentiment(new_net, text_nd)\n",
    "\n",
    "print('Probability for negative sentiment (0):  %0.4f ' % predictions.asnumpy()[0:1,0])\n",
    "print('Probability for positive sentiment (1):   %0.4f ' % predictions.asnumpy()[0:1,1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
